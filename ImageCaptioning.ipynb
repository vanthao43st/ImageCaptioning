{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanthao43st/ImageCaptioning/blob/main/ImageCaptioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4hpgUTfDR14",
        "outputId": "ffbdf52b-66c1-4a61-c099-2257b2b6f041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.keras.utils.set_random_seed(111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQOFlfyGDzJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c84ff5-e395-474c-fd20-81e8c30fa036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace Flicker8k_Dataset/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize parameters for the model"
      ],
      "metadata": {
        "id": "-rS7twSmVt2y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w00xOuH6KmVZ"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "SQL_LENGTH = 25\n",
        "\n",
        "EMBED_DIM = 512\n",
        "\n",
        "FF_DIM = 512\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the dataset"
      ],
      "metadata": {
        "id": "iLQadtSuWH_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfmONi_fSLes"
      },
      "outputs": [],
      "source": [
        "def load_caption_data(filename):\n",
        "    with open(filename) as f:\n",
        "        caption_data = f.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "        images_to_skip = set()\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_path = os.path.join(IMAGE_PATH, img_name.strip())\n",
        "\n",
        "            tokens = caption.strip().split()\n",
        "            if (len(tokens) < 5 or len(tokens) > SQL_LENGTH):\n",
        "                images_to_skip.add(img_path)\n",
        "                continue\n",
        "\n",
        "            if img_path.endswith(\"jpg\") and img_path not in images_to_skip:\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "                if img_path not in caption_mapping:\n",
        "                    caption_mapping[img_path] = list()\n",
        "                caption_mapping[img_path].append(caption)\n",
        "\n",
        "    for img_path in images_to_skip:\n",
        "        if img_path in caption_mapping:\n",
        "            del caption_mapping[img_path]\n",
        "\n",
        "    return caption_mapping, text_data\n",
        "\n",
        "caption_mapping, text_data = load_caption_data(\"Flickr8k.token.txt\")\n",
        "print(len(caption_mapping))\n",
        "print(caption_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap5MHmFzrSEn"
      },
      "outputs": [],
      "source": [
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    all_image_paths = list(caption_data.keys())\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_image_paths)\n",
        "\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "    train_ds = {\n",
        "        img_path: caption_data[img_path] for img_path in all_image_paths[:train_size]\n",
        "    }\n",
        "    val_ds = {\n",
        "        img_path: caption_data[img_path] for img_path in all_image_paths[train_size:]\n",
        "    }\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "train_ds, val_ds = train_val_split(caption_mapping)\n",
        "print(\"Number of training samples: \", len(train_ds))\n",
        "print(\"Number of validation samples: \", len(val_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The TextVectorization class normalizes, encodes, and vectorizes data."
      ],
      "metadata": {
        "id": "o9_I0ylCVA-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8PT8nqividx"
      },
      "outputs": [],
      "source": [
        "stripped_punctuation = string.punctuation.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(stripped_punctuation), \"\")\n",
        "\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SQL_LENGTH\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(text_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a67eSbNN0O21"
      },
      "source": [
        "##Size of vectorize_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RvnYpo30Nya"
      },
      "outputs": [],
      "source": [
        "len(vectorize_layer.get_vocabulary())\n",
        "vectorize_layer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylS8gHui1uc_"
      },
      "source": [
        "##Building a Dataset pipeline for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTFDaw5k1N07"
      },
      "outputs": [],
      "source": [
        "def decode_and_resize(img_path):\n",
        "    # Read image file as binary file\n",
        "    img = tf.io.read_file(img_path)\n",
        "\n",
        "    # jpeg image encoding with 3 color channels\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "    # Resize image to fit model\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "\n",
        "    # Normalize image pixels to within the range [0, 255]\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    return img\n",
        "\n",
        "def process_input_data(img_path, captions):\n",
        "    return decode_and_resize(img_path), vectorize_layer(captions)\n",
        "\n",
        "def make_dataset(img_path, captions):\n",
        "  # Create a TensorFlow dataset from two tensors: image paths and their corresponding captions\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_path, captions))\n",
        "\n",
        "    dataset = dataset.shuffle(BATCH_SIZE * 8)\n",
        "    dataset = dataset.map(process_input_data, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    # Prefetch the next batches while the current one is being processed, improving pipeline efficiency\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_dataset = make_dataset(list(train_ds.keys()), list(train_ds.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpA_wily0vS6"
      },
      "source": [
        "**Data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMWiXu1i0g1F"
      },
      "outputs": [],
      "source": [
        "image_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.3)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build model cnn and Transformer"
      ],
      "metadata": {
        "id": "MyTHL2EMVWuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build CNN model**"
      ],
      "metadata": {
        "id": "MvxoozRvtlgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cnn_model():\n",
        "    # Load the EfficientNetB0 model without the top classification layer, using pretrained ImageNet weights\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3),     # Input shape is (height, width, 3 channels for RGB)\n",
        "        include_top=False,                # Do not include the final dense classification layer\n",
        "        weights=\"imagenet\"                # Load pretrained weights from ImageNet\n",
        "    )\n",
        "\n",
        "    # Freeze all layers in the base model to prevent them from being updated during training\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Get the output tensor of the base model (feature maps)\n",
        "    base_model_out = base_model.output\n",
        "\n",
        "    # Reshape the 4D output (batch, height, width, channels) into 3D (batch, sequence_length, feature_dim)\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
        "\n",
        "    # Create a new model that maps input images to the reshaped feature vectors\n",
        "    cnn_model = tf.keras.models.Model(base_model.input, base_model_out)\n",
        "\n",
        "    return cnn_model\n",
        "\n"
      ],
      "metadata": {
        "id": "Ix6i0wmNtkfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# help(tf.keras.layers.Embedding)"
      ],
      "metadata": {
        "id": "16E460QKMGan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Transformer Architecture**"
      ],
      "metadata": {
        "id": "f21LCD07trpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_head, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim         # Store the embedding dimension (used for attention and projection layers)\n",
        "        self.dense_dim = dense_dim         # Used in the feed-forward network\n",
        "        self.num_head = num_head\n",
        "\n",
        "        # Initialize a Multi-Head Attention layer\n",
        "        # - num_heads: number of parallel attention heads\n",
        "        # - key_dim: dimensionality of each attention head (per head key/query vector)\n",
        "        # - dropout: dropout rate applied to attention weights (0.0 = no dropout)\n",
        "        self.multiHeadAttention = layers.MultiHeadAttention(\n",
        "            num_heads=num_head,\n",
        "            key_dim=embed_dim,\n",
        "            dropout=0.0\n",
        "        )\n",
        "\n",
        "        # First Layer Normalization layer, typically applied before/after attention block\n",
        "        self.layernormalization_1 = layers.LayerNormalization()\n",
        "\n",
        "        # Second Layer Normalization layer, typically used after the feed-forward network\n",
        "        self.layernormlization_2 = layers.LayerNormalization()\n",
        "\n",
        "        # First Dense layer in the position-wise feed-forward network, using ReLU activation\n",
        "        self.dense_1 = layers.Dense(dense_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs = self.layernormalization_1(inputs)\n",
        "        inputs = self.dense_1(inputs)\n",
        "        attention_output = self.multiHeadAttention(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=None,\n",
        "            training=training\n",
        "        )\n",
        "        out_1 = self.layernormlization_2(inputs + attention_output)\n",
        "        return out_1\n",
        "\n",
        "class PositionEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.token_embedding = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embed_dim,\n",
        "        )\n",
        "\n",
        "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
        "\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=sequence_length,\n",
        "            output_dim=embed_dim\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded_tokens = self.token_embedding(inputs)\n",
        "        embedded_tokens = embedded_tokens * self.embed_scale\n",
        "        length = tf.shape(inputs)[-1]                           # inputs.shape = (batch_size, sequence_length)\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embedding(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    # Automatically generate masks for elements in inputs, marking which are valid tokens and which are <PAD> (padding)\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.position_embedding = PositionEmbedding(\n",
        "            sequence_length=SQL_LENGTH,\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # First Multi-Head Self-Attention layer (Masked self-attention in the decoder)\n",
        "        self.multiHeadAttention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        # Second Multi-Head Attention layer (Encoder-Decoder attention)\n",
        "        self.multiHeadAttention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # First dense layer of the position-wise feed-forward network (FFN)\n",
        "        self.ffn_1 = layers.Dense(ff_dim, activation='relu')\n",
        "        # Second dense layer that projects back to embed_dim\n",
        "        self.ffn_2 = layers.Dense(embed_dim)\n",
        "\n",
        "        # Layer normalization after the first attention block\n",
        "        self.layernormalization_1 = layers.LayerNormalization()\n",
        "        # Layer normalization after the second attention block\n",
        "        self.layernormalization_2 = layers.LayerNormalization()\n",
        "        # Layer normalization after the feed-forward network (FFN)\n",
        "        self.layernormalization_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.dropout_1 = layers.Dropout(0.3)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "\n",
        "        self.out = layers.Dense(VOCAB_SIZE, activation='softmax')\n",
        "\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_causal_attention_mask(inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=tf.int32)\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult= tf.concat(\n",
        "            [\n",
        "                tf.expand_dims(batch_size, axis=-1),\n",
        "                tf.constant([1, 1], dtype=tf.int32)\n",
        "            ],\n",
        "            axis=0\n",
        "        )\n",
        "\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        # Convert token indices into embedding vectors (with positional encoding)\n",
        "        inputs = self.position_embedding(inputs)\n",
        "        # Create a causal mask to prevent attention to future tokens (for autoregressive decoding)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is not None:\n",
        "            # 2 different ways to expand the dimension\n",
        "            padding_mask = tf.cast(tf.expand_dims(mask, axis=-1), dtype=tf.int32)         # tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "\n",
        "            # Combine the padding mask with the causal mask for self-attention\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        # Multi-head self-attention with look-ahead masking (decoder attends to itself)\n",
        "        multiHeadAttention_output_1 = self.multiHeadAttention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        output_1 = self.layernormalization_1(inputs + multiHeadAttention_output_1)\n",
        "\n",
        "        # Multi-head cross-attention (decoder attends to encoder outputs)\n",
        "        multiHeadAttention_output_2 = self.multiHeadAttention_2(\n",
        "            query=output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        output_2 = self.layernormalization_2(output_1 + multiHeadAttention_output_2)\n",
        "\n",
        "        # Feed-forward network (position-wise)\n",
        "        ffn_output = self.ffn_1(output_2)\n",
        "        ffn_output = self.dropout_1(ffn_output, training=training)\n",
        "\n",
        "        # Project back to embedding dimension and apply residual connection + norm\n",
        "        ffn_output = self.ffn_2(ffn_output)\n",
        "        ffn_output = self.layernormalization_3(output_2 + ffn_output)\n",
        "\n",
        "        ffn_output = self.dropout_2(ffn_output, training=training)\n",
        "\n",
        "        # Project to vocabulary space using final softmax layer to predict next token\n",
        "        predictions = self.out(ffn_output)\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "y-XehN_UnOdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##View constructor and call properties of available layers"
      ],
      "metadata": {
        "id": "AoFQIZf0Xqex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(tf.keras.layers.Dropout.call)\n",
        "# help(tf.keras.layers.Dropout)\n",
        "help(tf.keras.Model.loss)"
      ],
      "metadata": {
        "id": "-Zu-eIsYs2ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Design image captioning model"
      ],
      "metadata": {
        "id": "56JOMH1fYggD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cnn_model,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        num_captions_per_image=5,\n",
        "        image_aug=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "        self.image_aug = image_aug\n",
        "\n",
        "        # Track average loss and accuracy after each batch.\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "    def caculate_loss(self, y_true, y_pred, mask):\n",
        "        # Compute the raw loss (e.g., categorical crossentropy) between true and predicted values\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        # Apply the mask to the loss — only keep losses at valid (non-padded) positions\n",
        "        loss *= mask\n",
        "        # Return the average loss over only the valid (non-masked) positions\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def caculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=accuracy.dtype)\n",
        "        accuracy *= mask\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def compute_loss_and_acc(self, img_embed, batch_sequence, training=True):\n",
        "        # Pass image embeddings through the encoder\n",
        "        encoder_output = self.encoder(img_embed, training=training)\n",
        "\n",
        "        # Prepare decoder inputs by shifting the sequence one step to the right\n",
        "        batch_sequence_input = batch_sequence[:, :-1]       # Input to the decoder (start tokens to last-1)\n",
        "        batch_sequence_true = batch_sequence[:, 1:]         # True target sequence (second token to end)\n",
        "\n",
        "        # Create a mask to ignore padding tokens (token ID = 0)\n",
        "        mask = tf.math.not_equal(batch_sequence_true, 0)\n",
        "\n",
        "        # Run the decoder to get predictions for the next tokens\n",
        "        batch_sequence_prediction = self.decoder(\n",
        "            batch_sequence_input,\n",
        "            encoder_output,\n",
        "            training=training,\n",
        "            mask=mask\n",
        "        )\n",
        "\n",
        "        loss = self.caculate_loss(batch_sequence_true, batch_sequence_prediction, mask)\n",
        "        acc = self.caculate_accuracy(batch_sequence_true, batch_sequence_prediction, mask)\n",
        "\n",
        "        return loss, acc\n",
        "\n",
        "\n",
        "    # Override Train & Test Step\n",
        "    def train_step(self, batch_data):\n",
        "        batch_image, batch_sequence = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        if self.image_aug:\n",
        "            batch_image = self.image_aug(batch_image)\n",
        "\n",
        "        # Get image embeddings\n",
        "        img_embed = self.cnn_model(batch_image)\n",
        "\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            # Tape sẽ ghi lại toàn bộ quá trình tính loss: qua encoder, decoder, attention, embedding.\n",
        "            # Ghi lại các phép toán để sau đó tự động tính đạo hàm phục vụ tối ưu. Mọi phép toán từ đây sẽ được \"ghi nhớ\" để backpropagate.\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, acc = self.compute_loss_and_acc(\n",
        "                    img_embed,\n",
        "                    batch_sequence[:, i, :],      # (batch_size, num_captions_per_image, sequence_length)\n",
        "                    training=True\n",
        "                )\n",
        "\n",
        "                # Update loss and accuracy\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # Get all weights to train\n",
        "            train_vars = (self.encoder.trainable_variables + self.decoder.trainable_variables)\n",
        "\n",
        "            # Take the derivative of loss with respect to each weight\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # zip(grads, train_vars) generates each pair (gradient, weight)\n",
        "            # apply_gradients() will update each weight using the formula: W_new = W_old - learning_rate * grad\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"accuracy\": self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        batch_image, batch_sequence = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        img_embed = self.cnn_model(batch_image)\n",
        "        for i in range(self.num_captions_per_image):\n",
        "            loss, acc = self.compute_loss_and_acc(\n",
        "                img_embed,\n",
        "                batch_sequence[:, i, :],\n",
        "                training=False\n",
        "            )\n",
        "\n",
        "            batch_loss += loss\n",
        "            batch_acc += acc\n",
        "\n",
        "        batch_acc /= float(self.num_captions_per_image)\n",
        "        self.loss_tracker.update_state(batch_loss)\n",
        "        self.acc_tracker.update_state(batch_acc)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.loss_tracker.result(),\n",
        "            \"accuracy\": self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    # Define the list of metrics to track during training and evaluation\n",
        "    # This allows Keras to automatically call `reset_states()` on each metric between epochs\n",
        "    # and to include them in logs (e.g., in model.fit and model.evaluate)\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n"
      ],
      "metadata": {
        "id": "0U8ADR0uXl_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = get_cnn_model()\n",
        "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_head=1)\n",
        "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    num_captions_per_image=5,\n",
        "    image_aug=image_augmentation\n",
        ")"
      ],
      "metadata": {
        "id": "_PtHILPihiDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "wBZZ3anByFYF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3TpqmMBfWNy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}